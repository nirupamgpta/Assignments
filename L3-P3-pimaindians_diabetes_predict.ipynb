{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "NscuND4zhCV2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (Pima Indians Diabetes dataset)\n",
        "# You can download the dataset from 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n",
        "# Assuming you have it in CSV format\n",
        "# Column Names: ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
        "           'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=columns)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4XR98p_hIdv",
        "outputId": "c6541771-85c7-492e-b366-be4821c88f42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n",
            "\n",
            "[768 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and labels (y)\n",
        "X = data.iloc[:, :-1].values  # All columns except the last one are features\n",
        "y = data.iloc[:, -1].values   # The last column is the target (Outcome: 0 or 1)\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7Np0LtxDhMA2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcZrb8eag-N2",
        "outputId": "93695859-fcf5-45b6-fe18-b070e3ebc5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4130 - loss: 0.7309 - val_accuracy: 0.7013 - val_loss: 0.6549\n",
            "Epoch 2/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6896 - loss: 0.6319 - val_accuracy: 0.6948 - val_loss: 0.5993\n",
            "Epoch 3/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7005 - loss: 0.5648 - val_accuracy: 0.7468 - val_loss: 0.5588\n",
            "Epoch 4/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7480 - loss: 0.5265 - val_accuracy: 0.7532 - val_loss: 0.5353\n",
            "Epoch 5/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7730 - loss: 0.4691 - val_accuracy: 0.7727 - val_loss: 0.5224\n",
            "Epoch 6/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7909 - loss: 0.4537 - val_accuracy: 0.7662 - val_loss: 0.5162\n",
            "Epoch 7/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8045 - loss: 0.4366 - val_accuracy: 0.7597 - val_loss: 0.5129\n",
            "Epoch 8/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7744 - loss: 0.4934 - val_accuracy: 0.7532 - val_loss: 0.5147\n",
            "Epoch 9/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7824 - loss: 0.4543 - val_accuracy: 0.7662 - val_loss: 0.5145\n",
            "Epoch 10/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7946 - loss: 0.4226 - val_accuracy: 0.7662 - val_loss: 0.5129\n",
            "Epoch 11/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7683 - loss: 0.4825 - val_accuracy: 0.7597 - val_loss: 0.5137\n",
            "Epoch 12/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7981 - loss: 0.4227 - val_accuracy: 0.7597 - val_loss: 0.5140\n",
            "Epoch 13/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7758 - loss: 0.4545 - val_accuracy: 0.7532 - val_loss: 0.5144\n",
            "Epoch 14/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7789 - loss: 0.4322 - val_accuracy: 0.7532 - val_loss: 0.5179\n",
            "Epoch 15/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8142 - loss: 0.3889 - val_accuracy: 0.7532 - val_loss: 0.5166\n",
            "Epoch 16/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7809 - loss: 0.4427 - val_accuracy: 0.7468 - val_loss: 0.5179\n",
            "Epoch 17/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7788 - loss: 0.4320 - val_accuracy: 0.7532 - val_loss: 0.5200\n",
            "Epoch 18/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8057 - loss: 0.4035 - val_accuracy: 0.7532 - val_loss: 0.5210\n",
            "Epoch 19/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7695 - loss: 0.4316 - val_accuracy: 0.7597 - val_loss: 0.5230\n",
            "Epoch 20/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7838 - loss: 0.4145 - val_accuracy: 0.7532 - val_loss: 0.5243\n",
            "Epoch 21/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7792 - loss: 0.4194 - val_accuracy: 0.7532 - val_loss: 0.5278\n",
            "Epoch 22/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7979 - loss: 0.4257 - val_accuracy: 0.7532 - val_loss: 0.5261\n",
            "Epoch 23/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7755 - loss: 0.4181 - val_accuracy: 0.7597 - val_loss: 0.5278\n",
            "Epoch 24/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7908 - loss: 0.4262 - val_accuracy: 0.7597 - val_loss: 0.5305\n",
            "Epoch 25/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7835 - loss: 0.4311 - val_accuracy: 0.7597 - val_loss: 0.5321\n",
            "Epoch 26/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8096 - loss: 0.3803 - val_accuracy: 0.7597 - val_loss: 0.5295\n",
            "Epoch 27/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7785 - loss: 0.4304 - val_accuracy: 0.7597 - val_loss: 0.5315\n",
            "Epoch 28/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7810 - loss: 0.4340 - val_accuracy: 0.7727 - val_loss: 0.5356\n",
            "Epoch 29/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7850 - loss: 0.4273 - val_accuracy: 0.7597 - val_loss: 0.5389\n",
            "Epoch 30/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8145 - loss: 0.3654 - val_accuracy: 0.7532 - val_loss: 0.5408\n",
            "Epoch 31/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8018 - loss: 0.4045 - val_accuracy: 0.7597 - val_loss: 0.5415\n",
            "Epoch 32/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7966 - loss: 0.4065 - val_accuracy: 0.7662 - val_loss: 0.5428\n",
            "Epoch 33/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7998 - loss: 0.4063 - val_accuracy: 0.7662 - val_loss: 0.5450\n",
            "Epoch 34/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8126 - loss: 0.3826 - val_accuracy: 0.7662 - val_loss: 0.5463\n",
            "Epoch 35/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8154 - loss: 0.4061 - val_accuracy: 0.7597 - val_loss: 0.5477\n",
            "Epoch 36/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7763 - loss: 0.4312 - val_accuracy: 0.7662 - val_loss: 0.5506\n",
            "Epoch 37/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8028 - loss: 0.4102 - val_accuracy: 0.7662 - val_loss: 0.5536\n",
            "Epoch 38/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8390 - loss: 0.3703 - val_accuracy: 0.7662 - val_loss: 0.5541\n",
            "Epoch 39/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8264 - loss: 0.3744 - val_accuracy: 0.7662 - val_loss: 0.5546\n",
            "Epoch 40/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7937 - loss: 0.4282 - val_accuracy: 0.7662 - val_loss: 0.5588\n",
            "Epoch 41/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8242 - loss: 0.3726 - val_accuracy: 0.7662 - val_loss: 0.5592\n",
            "Epoch 42/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8023 - loss: 0.3912 - val_accuracy: 0.7662 - val_loss: 0.5620\n",
            "Epoch 43/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8287 - loss: 0.4001 - val_accuracy: 0.7662 - val_loss: 0.5638\n",
            "Epoch 44/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8279 - loss: 0.3548 - val_accuracy: 0.7662 - val_loss: 0.5659\n",
            "Epoch 45/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8182 - loss: 0.3877 - val_accuracy: 0.7662 - val_loss: 0.5638\n",
            "Epoch 46/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7767 - loss: 0.4042 - val_accuracy: 0.7662 - val_loss: 0.5662\n",
            "Epoch 47/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8263 - loss: 0.3819 - val_accuracy: 0.7727 - val_loss: 0.5675\n",
            "Epoch 48/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7874 - loss: 0.4233 - val_accuracy: 0.7662 - val_loss: 0.5704\n",
            "Epoch 49/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8189 - loss: 0.3822 - val_accuracy: 0.7727 - val_loss: 0.5737\n",
            "Epoch 50/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8188 - loss: 0.3839 - val_accuracy: 0.7727 - val_loss: 0.5742\n",
            "Epoch 51/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8360 - loss: 0.3563 - val_accuracy: 0.7597 - val_loss: 0.5762\n",
            "Epoch 52/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8365 - loss: 0.3769 - val_accuracy: 0.7597 - val_loss: 0.5770\n",
            "Epoch 53/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8297 - loss: 0.3858 - val_accuracy: 0.7662 - val_loss: 0.5748\n",
            "Epoch 54/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8441 - loss: 0.3695 - val_accuracy: 0.7468 - val_loss: 0.5818\n",
            "Epoch 55/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8317 - loss: 0.3777 - val_accuracy: 0.7532 - val_loss: 0.5795\n",
            "Epoch 56/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8234 - loss: 0.3699 - val_accuracy: 0.7597 - val_loss: 0.5803\n",
            "Epoch 57/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8491 - loss: 0.3576 - val_accuracy: 0.7532 - val_loss: 0.5788\n",
            "Epoch 58/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8345 - loss: 0.3764 - val_accuracy: 0.7532 - val_loss: 0.5787\n",
            "Epoch 59/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8242 - loss: 0.3859 - val_accuracy: 0.7532 - val_loss: 0.5795\n",
            "Epoch 60/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8570 - loss: 0.3551 - val_accuracy: 0.7532 - val_loss: 0.5830\n",
            "Epoch 61/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8177 - loss: 0.3741 - val_accuracy: 0.7532 - val_loss: 0.5842\n",
            "Epoch 62/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7970 - loss: 0.4092 - val_accuracy: 0.7597 - val_loss: 0.5819\n",
            "Epoch 63/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8021 - loss: 0.4239 - val_accuracy: 0.7597 - val_loss: 0.5853\n",
            "Epoch 64/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8355 - loss: 0.3647 - val_accuracy: 0.7468 - val_loss: 0.5863\n",
            "Epoch 65/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7842 - loss: 0.4270 - val_accuracy: 0.7468 - val_loss: 0.5865\n",
            "Epoch 66/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8303 - loss: 0.3839 - val_accuracy: 0.7468 - val_loss: 0.5864\n",
            "Epoch 67/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8201 - loss: 0.3816 - val_accuracy: 0.7532 - val_loss: 0.5896\n",
            "Epoch 68/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8432 - loss: 0.3799 - val_accuracy: 0.7532 - val_loss: 0.5855\n",
            "Epoch 69/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8472 - loss: 0.3551 - val_accuracy: 0.7403 - val_loss: 0.5900\n",
            "Epoch 70/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8552 - loss: 0.3423 - val_accuracy: 0.7403 - val_loss: 0.5892\n",
            "Epoch 71/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8383 - loss: 0.3724 - val_accuracy: 0.7403 - val_loss: 0.5933\n",
            "Epoch 72/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8105 - loss: 0.4019 - val_accuracy: 0.7338 - val_loss: 0.5895\n",
            "Epoch 73/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8393 - loss: 0.3782 - val_accuracy: 0.7468 - val_loss: 0.5944\n",
            "Epoch 74/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8507 - loss: 0.3447 - val_accuracy: 0.7468 - val_loss: 0.5934\n",
            "Epoch 75/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8304 - loss: 0.3678 - val_accuracy: 0.7403 - val_loss: 0.5949\n",
            "Epoch 76/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8477 - loss: 0.3642 - val_accuracy: 0.7403 - val_loss: 0.5884\n",
            "Epoch 77/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8482 - loss: 0.3354 - val_accuracy: 0.7403 - val_loss: 0.5932\n",
            "Epoch 78/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8345 - loss: 0.3680 - val_accuracy: 0.7338 - val_loss: 0.5883\n",
            "Epoch 79/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8365 - loss: 0.3749 - val_accuracy: 0.7403 - val_loss: 0.5969\n",
            "Epoch 80/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8333 - loss: 0.3381 - val_accuracy: 0.7403 - val_loss: 0.5947\n",
            "Epoch 81/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8345 - loss: 0.3596 - val_accuracy: 0.7338 - val_loss: 0.5927\n",
            "Epoch 82/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8406 - loss: 0.3519 - val_accuracy: 0.7273 - val_loss: 0.5930\n",
            "Epoch 83/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8484 - loss: 0.3410 - val_accuracy: 0.7403 - val_loss: 0.5951\n",
            "Epoch 84/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8467 - loss: 0.3417 - val_accuracy: 0.7403 - val_loss: 0.5962\n",
            "Epoch 85/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8460 - loss: 0.3461 - val_accuracy: 0.7338 - val_loss: 0.5940\n",
            "Epoch 86/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8320 - loss: 0.3762 - val_accuracy: 0.7273 - val_loss: 0.5929\n",
            "Epoch 87/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8395 - loss: 0.3558 - val_accuracy: 0.7338 - val_loss: 0.6029\n",
            "Epoch 88/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8168 - loss: 0.3824 - val_accuracy: 0.7273 - val_loss: 0.5982\n",
            "Epoch 89/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8110 - loss: 0.3941 - val_accuracy: 0.7338 - val_loss: 0.5996\n",
            "Epoch 90/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8449 - loss: 0.3256 - val_accuracy: 0.7338 - val_loss: 0.5998\n",
            "Epoch 91/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8568 - loss: 0.3441 - val_accuracy: 0.7403 - val_loss: 0.6053\n",
            "Epoch 92/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8366 - loss: 0.3722 - val_accuracy: 0.7338 - val_loss: 0.6027\n",
            "Epoch 93/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8338 - loss: 0.3558 - val_accuracy: 0.7273 - val_loss: 0.5982\n",
            "Epoch 94/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8711 - loss: 0.3205 - val_accuracy: 0.7403 - val_loss: 0.6006\n",
            "Epoch 95/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8244 - loss: 0.3765 - val_accuracy: 0.7338 - val_loss: 0.5980\n",
            "Epoch 96/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8389 - loss: 0.3515 - val_accuracy: 0.7403 - val_loss: 0.6008\n",
            "Epoch 97/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8573 - loss: 0.3544 - val_accuracy: 0.7273 - val_loss: 0.6026\n",
            "Epoch 98/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8327 - loss: 0.3718 - val_accuracy: 0.7273 - val_loss: 0.6016\n",
            "Epoch 99/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8099 - loss: 0.3965 - val_accuracy: 0.7403 - val_loss: 0.6048\n",
            "Epoch 100/100\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8268 - loss: 0.3517 - val_accuracy: 0.7273 - val_loss: 0.5995\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa2a6cfefc0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding layers\n",
        "model.add(Dense(16, input_dim=8, activation='relu'))  # First hidden layer with 16 neurons, input size is 8 features\n",
        "model.add(Dense(12, activation='relu'))               # Second hidden layer with 12 neurons\n",
        "model.add(Dense(1, activation='sigmoid'))             # Output layer with 1 neuron (for binary classification)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Predict diabetes for new samples (3 new patients)\n",
        "samples = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50],   # Sample 1\n",
        "                    [1, 85, 66, 29, 0, 26.6, 0.351, 31],    # Sample 2\n",
        "                    [8, 183, 64, 0, 0, 23.3, 0.672, 32]])   # Sample 3\n",
        "\n",
        "# Standardize the new data using the same scaler\n",
        "samples_scaled = scaler.transform(samples)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Bfy96lhXsV",
        "outputId": "b1cf9e54-ecd4-4b26-dfc1-f9d4fc31ab7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7212 - loss: 0.5788\n",
            "Test Accuracy: 72.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict diabetes (returns probabilities)\n",
        "predictions = model.predict(samples_scaled)\n",
        "\n",
        "# Convert probabilities to class labels (0 or 1)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Output predictions\n",
        "print(\"Predictions for the samples (0 = No Diabetes, 1 = Diabetes):\")\n",
        "print(predicted_classes.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ISF-0ychloO",
        "outputId": "8403046b-4ae7-46fe-f3b9-38f5e4529c8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "Predictions for the samples (0 = No Diabetes, 1 = Diabetes):\n",
            "[1 0 1]\n"
          ]
        }
      ]
    }
  ]
}